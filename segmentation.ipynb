{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder\n",
    "\n",
    "import timm\n",
    "from timm.models.layers import Conv2dSame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3dSame(nn.Conv3d):\n",
    "    \"\"\" Tensorflow like 'SAME' convolution wrapper for 3D convolutions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            groups=1,\n",
    "            bias=True,\n",
    "    ):\n",
    "        # Calculate padding for SAME behavior\n",
    "        if isinstance(kernel_size, int):\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        elif isinstance(kernel_size, (tuple, list)):\n",
    "            padding = [(k - 1) // 2 for k in kernel_size]\n",
    "        else:\n",
    "            raise ValueError(\"kernel_size must be int or iterable of int\")\n",
    "\n",
    "        super(Conv3dSame, self).__init__(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride, padding, dilation, groups, bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return conv3d_same(\n",
    "            x, self.weight, self.bias,\n",
    "            self.stride, self.padding, self.dilation, self.groups,\n",
    "        )\n",
    "\n",
    "def conv3d_same(x, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    # Here you would need to implement or call the actual 3D convolution logic\n",
    "    # with SAME padding, as PyTorch's native convolution does not have this padding mode.\n",
    "    # This is a placeholder implementation and should be replaced with actual logic.\n",
    "    conv = nn.functional.conv3d(\n",
    "        x, weight, bias, stride, padding, dilation, groups\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = 'data'\n",
    "segmentations_path = os.path.join(base_path, 'segmentations')\n",
    "train_images_path1 = os.path.join(base_path, 'sfd', 'train_images')\n",
    "train_images_path2 = os.path.join(base_path, 'train_images')\n",
    "output_path = os.path.join(base_path, 'processed_3d')\n",
    "\n",
    "backbone='resnet18d'\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "n_blocks = 4\n",
    "out_dim = 7\n",
    "loss_weights = [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_image_volume(study_path, target_size=(128, 128, 128)):\n",
    "    # List and sort slice file paths\n",
    "    t_paths = sorted(glob(os.path.join(study_path, \"*\")),\n",
    "                     key=lambda x: int(os.path.basename(x).split('.')[0]))\n",
    "    \n",
    "    # Determine the number of scans and calculate quantile indices\n",
    "    n_scans = len(t_paths)\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., target_size[2])).round().astype(int)\n",
    "    t_paths = [t_paths[i] for i in indices]\n",
    "    \n",
    "    # Load and process slices\n",
    "    slices = []\n",
    "    for img_path in t_paths:\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((target_size[0], target_size[1]))\n",
    "        slices.append(np.array(img))\n",
    "    \n",
    "    # Stack slices into a 3D volume\n",
    "    volume = np.stack(slices, axis=-1)\n",
    "    \n",
    "    # Normalize and scale the volume\n",
    "    volume = volume - np.min(volume)\n",
    "    volume = volume / (np.max(volume) + 1e-4)\n",
    "    volume = (volume * 255).astype(np.uint8)\n",
    "    \n",
    "    return volume\n",
    "\n",
    "def load_and_process_mask(mask_path, target_size=(128, 128, 128), num_classes=7):\n",
    "    # Load the mask using nibabel\n",
    "    mask_org = nib.load(mask_path).get_fdata()\n",
    "\n",
    "    # Adjust mask orientation if needed\n",
    "    mask_org = mask_org.transpose(1, 0, 2)[::-1, :, ::-1]  # Adjust orientation to (d, w, h)\n",
    "\n",
    "    # Resize mask to target size\n",
    "    if mask_org.shape != target_size:\n",
    "        factors = [t / s for t, s in zip(target_size, mask_org.shape)]\n",
    "        mask_org = zoom(mask_org, factors, order=0)  # Nearest-neighbor interpolation for masks\n",
    "\n",
    "    # Create multi-channel mask\n",
    "    mask = np.zeros((num_classes, target_size[0], target_size[1], target_size[2]))\n",
    "    for cid in range(num_classes):\n",
    "        mask[cid] = (mask_org == (cid + 1))\n",
    "\n",
    "    # Convert mask to [0, 255] and return as uint8\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    # Get list of study IDs with segmentations\n",
    "    segmentation_ids = [f.split('.')[:-1] for f in os.listdir(segmentations_path) if f.endswith('.nii')]\n",
    "\n",
    "    for study_id_parts in tqdm(segmentation_ids, desc=\"Processing studies\"):\n",
    "        study_id = '.'.join(study_id_parts)\n",
    "        # Check for study folder in both locations\n",
    "        study_path = None\n",
    "        if os.path.exists(os.path.join(train_images_path1, study_id)):\n",
    "            study_path = os.path.join(train_images_path1, study_id)\n",
    "        elif os.path.exists(os.path.join(train_images_path2, study_id)):\n",
    "            study_path = os.path.join(train_images_path2, study_id)\n",
    "        \n",
    "        if study_path is None:\n",
    "            print(f\"Warning: No image folder found for study {study_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Process image volume\n",
    "        image_volume = load_and_process_image_volume(study_path)\n",
    "        \n",
    "        # Process mask\n",
    "        mask_path = os.path.join(segmentations_path, f\"{study_id}.nii\")\n",
    "        mask_volume = load_and_process_mask(mask_path)\n",
    "        \n",
    "        # Save processed data\n",
    "        np.save(os.path.join(output_path, f\"{study_id}_image.npy\"), image_volume)\n",
    "        np.save(os.path.join(output_path, f\"{study_id}_mask.npy\"), mask_volume)\n",
    "\n",
    "    print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 128, 128, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "m = TimmSegModel(backbone)\n",
    "m = convert_3d(m)\n",
    "m(torch.rand(1, 3, 128,128,128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m     intersection \u001b[38;5;241m=\u001b[39m (iflat \u001b[38;5;241m*\u001b[39m tflat)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ((\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m intersection \u001b[38;5;241m+\u001b[39m smooth) \u001b[38;5;241m/\u001b[39m (iflat\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m tflat\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m+\u001b[39m smooth))\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbce_dice\u001b[39m(\u001b[38;5;28minput\u001b[39m, target, loss_weights\u001b[38;5;241m=\u001b[39m\u001b[43mloss_weights\u001b[49m):\n\u001b[0;32m     60\u001b[0m     loss1 \u001b[38;5;241m=\u001b[39m loss_weights[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m     61\u001b[0m     loss2 \u001b[38;5;241m=\u001b[39m loss_weights[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m dice_loss(\u001b[38;5;28minput\u001b[39m, target)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_weights' is not defined"
     ]
    }
   ],
   "source": [
    "def binary_dice_score(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    threshold: Optional[float] = None,\n",
    "    nan_score_on_empty=False,\n",
    "    eps: float = 1e-7,\n",
    ") -> float:\n",
    "\n",
    "    if threshold is not None:\n",
    "        y_pred = (y_pred > threshold).to(y_true.dtype)\n",
    "\n",
    "    intersection = torch.sum(y_pred * y_true).item()\n",
    "    cardinality = (torch.sum(y_pred) + torch.sum(y_true)).item()\n",
    "\n",
    "    score = (2.0 * intersection) / (cardinality + eps)\n",
    "\n",
    "    has_targets = torch.sum(y_true) > 0\n",
    "    has_predicted = torch.sum(y_pred) > 0\n",
    "\n",
    "    if not has_targets:\n",
    "        if nan_score_on_empty:\n",
    "            score = np.nan\n",
    "        else:\n",
    "            score = float(not has_predicted)\n",
    "    return score\n",
    "\n",
    "\n",
    "def multilabel_dice_score(\n",
    "    y_true: torch.Tensor,\n",
    "    y_pred: torch.Tensor,\n",
    "    threshold=None,\n",
    "    eps=1e-7,\n",
    "    nan_score_on_empty=False,\n",
    "):\n",
    "    ious = []\n",
    "    num_classes = y_pred.size(0)\n",
    "    for class_index in range(num_classes):\n",
    "        iou = binary_dice_score(\n",
    "            y_pred=y_pred[class_index],\n",
    "            y_true=y_true[class_index],\n",
    "            threshold=threshold,\n",
    "            nan_score_on_empty=nan_score_on_empty,\n",
    "            eps=eps,\n",
    "        )\n",
    "        ious.append(iou)\n",
    "\n",
    "    return ious\n",
    "\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "def bce_dice(input, target, loss_weights=loss_weights):\n",
    "    loss1 = loss_weights[0] * nn.BCEWithLogitsLoss()(input, target)\n",
    "    loss2 = loss_weights[1] * dice_loss(input, target)\n",
    "    return (loss1 + loss2) / sum(loss_weights)\n",
    "\n",
    "criterion = bce_dice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
