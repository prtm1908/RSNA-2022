{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder\n",
    "\n",
    "from monai.transforms import Resize\n",
    "import  monai.transforms as transforms\n",
    "\n",
    "import timm\n",
    "from timm.models.layers import Conv2dSame\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3dSame(nn.Conv3d):\n",
    "    \"\"\" Tensorflow like 'SAME' convolution wrapper for 3D convolutions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            groups=1,\n",
    "            bias=True,\n",
    "    ):\n",
    "        # Calculate padding for SAME behavior\n",
    "        if isinstance(kernel_size, int):\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        elif isinstance(kernel_size, (tuple, list)):\n",
    "            padding = [(k - 1) // 2 for k in kernel_size]\n",
    "        else:\n",
    "            raise ValueError(\"kernel_size must be int or iterable of int\")\n",
    "\n",
    "        super(Conv3dSame, self).__init__(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride, padding, dilation, groups, bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return conv3d_same(\n",
    "            x, self.weight, self.bias,\n",
    "            self.stride, self.padding, self.dilation, self.groups,\n",
    "        )\n",
    "\n",
    "def conv3d_same(x, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    # Here you would need to implement or call the actual 3D convolution logic\n",
    "    # with SAME padding, as PyTorch's native convolution does not have this padding mode.\n",
    "    # This is a placeholder implementation and should be replaced with actual logic.\n",
    "    conv = nn.functional.conv3d(\n",
    "        x, weight, bias, stride, padding, dilation, groups\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = 'data'\n",
    "segmentations_path = os.path.join(base_path, 'segmentations')\n",
    "train_images_path1 = os.path.join(base_path, 'sfd', 'train_images')\n",
    "train_images_path2 = os.path.join(base_path, 'train_images')\n",
    "output_path = os.path.join(base_path, 'processed_3d')\n",
    "\n",
    "kernel_type = 'timm3d_res18d_unet4b_128_128_128_dsv2_flip12_shift333p7_gd1p5_bs4_lr3e4_20x50ep'\n",
    "load_kernel = None\n",
    "load_last = True\n",
    "n_blocks = 4\n",
    "n_folds = 5\n",
    "backbone = 'resnet18d'\n",
    "\n",
    "image_sizes = [128, 128, 128]\n",
    "R = Resize(image_sizes)\n",
    "\n",
    "init_lr = 3e-3\n",
    "batch_size = 4\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "loss_weights = [1, 1]\n",
    "p_mixup = 0.1\n",
    "\n",
    "data_dir = output_path\n",
    "use_amp = True\n",
    "num_workers = 4\n",
    "out_dim = 7\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "log_dir = './logs'\n",
    "model_dir = './models'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_image_volume(study_path, target_size=(128, 128, 128)):\n",
    "    # List and sort slice file paths\n",
    "    t_paths = sorted(glob(os.path.join(study_path, \"*\")),\n",
    "                     key=lambda x: int(os.path.basename(x).split('.')[0]))\n",
    "    \n",
    "    # Determine the number of scans and calculate quantile indices\n",
    "    n_scans = len(t_paths)\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., target_size[2])).round().astype(int)\n",
    "    t_paths = [t_paths[i] for i in indices]\n",
    "    \n",
    "    # Load and process slices\n",
    "    slices = []\n",
    "    for img_path in t_paths:\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((target_size[0], target_size[1]))\n",
    "        slices.append(np.array(img))\n",
    "    \n",
    "    # Stack slices into a 3D volume\n",
    "    volume = np.stack(slices, axis=-1)\n",
    "    \n",
    "    # Normalize and scale the volume\n",
    "    volume = volume - np.min(volume)\n",
    "    volume = volume / (np.max(volume) + 1e-4)\n",
    "    volume = (volume * 255).astype(np.uint8)\n",
    "    \n",
    "    return volume\n",
    "\n",
    "def load_and_process_mask(mask_path, target_size=(128, 128, 128), num_classes=7):\n",
    "    # Load the mask using nibabel\n",
    "    mask_org = nib.load(mask_path).get_fdata()\n",
    "\n",
    "    # Adjust mask orientation if needed\n",
    "    mask_org = mask_org.transpose(1, 0, 2)[::-1, :, ::-1]  # Adjust orientation to (d, w, h)\n",
    "\n",
    "    # Resize mask to target size\n",
    "    if mask_org.shape != target_size:\n",
    "        factors = [t / s for t, s in zip(target_size, mask_org.shape)]\n",
    "        mask_org = zoom(mask_org, factors, order=0)  # Nearest-neighbor interpolation for masks\n",
    "\n",
    "    # Create multi-channel mask\n",
    "    mask = np.zeros((num_classes, target_size[0], target_size[1], target_size[2]))\n",
    "    for cid in range(num_classes):\n",
    "        mask[cid] = (mask_org == (cid + 1))\n",
    "\n",
    "    # Convert mask to [0, 255] and return as uint8\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    # Get list of study IDs with segmentations\n",
    "    segmentation_ids = [f.split('.')[:-1] for f in os.listdir(segmentations_path) if f.endswith('.nii')]\n",
    "\n",
    "    for study_id_parts in tqdm(segmentation_ids, desc=\"Processing studies\"):\n",
    "        study_id = '.'.join(study_id_parts)\n",
    "        # Check for study folder in both locations\n",
    "        study_path = None\n",
    "        if os.path.exists(os.path.join(train_images_path1, study_id)):\n",
    "            study_path = os.path.join(train_images_path1, study_id)\n",
    "        elif os.path.exists(os.path.join(train_images_path2, study_id)):\n",
    "            study_path = os.path.join(train_images_path2, study_id)\n",
    "        \n",
    "        if study_path is None:\n",
    "            print(f\"Warning: No image folder found for study {study_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Process image volume\n",
    "        image_volume = load_and_process_image_volume(study_path)\n",
    "        \n",
    "        # Process mask\n",
    "        mask_path = os.path.join(segmentations_path, f\"{study_id}.nii\")\n",
    "        mask_volume = load_and_process_mask(mask_path)\n",
    "        \n",
    "        # Save processed data\n",
    "        np.save(os.path.join(output_path, f\"{study_id}_image.npy\"), image_volume)\n",
    "        np.save(os.path.join(output_path, f\"{study_id}_mask.npy\"), mask_volume)\n",
    "\n",
    "    print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              slice_path  \\\n",
      "29827  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
      "29828  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
      "29829  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
      "29830  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
      "29831  ../input/rsna-2022-cervical-spine-fracture-det...   \n",
      "\n",
      "                StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7  \\\n",
      "29827  1.2.826.0.1.3680043.30524                1   0   0   0   0   0   1   1   \n",
      "29828  1.2.826.0.1.3680043.30524                1   0   0   0   0   0   1   1   \n",
      "29829  1.2.826.0.1.3680043.30524                1   0   0   0   0   0   1   1   \n",
      "29830  1.2.826.0.1.3680043.30524                1   0   0   0   0   0   1   1   \n",
      "29831  1.2.826.0.1.3680043.30524                1   0   0   0   0   0   1   1   \n",
      "\n",
      "       slice_id  width  height  \\\n",
      "29827        26    512     512   \n",
      "29828       185    512     512   \n",
      "29829       221    512     512   \n",
      "29830        13    512     512   \n",
      "29831       140    512     512   \n",
      "\n",
      "                                               mask_file  \\\n",
      "29827  data\\processed_3d\\1.2.826.0.1.3680043.30524_ma...   \n",
      "29828  data\\processed_3d\\1.2.826.0.1.3680043.30524_ma...   \n",
      "29829  data\\processed_3d\\1.2.826.0.1.3680043.30524_ma...   \n",
      "29830  data\\processed_3d\\1.2.826.0.1.3680043.30524_ma...   \n",
      "29831  data\\processed_3d\\1.2.826.0.1.3680043.30524_ma...   \n",
      "\n",
      "                                      image_folder  fold  \n",
      "29827  data\\train_images\\1.2.826.0.1.3680043.30524     1  \n",
      "29828  data\\train_images\\1.2.826.0.1.3680043.30524     4  \n",
      "29829  data\\train_images\\1.2.826.0.1.3680043.30524     1  \n",
      "29830  data\\train_images\\1.2.826.0.1.3680043.30524     4  \n",
      "29831  data\\train_images\\1.2.826.0.1.3680043.30524     2  \n"
     ]
    }
   ],
   "source": [
    "# Define the data directory\n",
    "data_dir_parent = 'data'\n",
    "\n",
    "# Read the training data\n",
    "df_train = pd.read_csv(os.path.join(data_dir_parent, 'train.csv'))\n",
    "\n",
    "# Get the list of mask files\n",
    "mask_files = [f for f in os.listdir(os.path.join(data_dir_parent, 'processed_3d')) if f.endswith('_mask.npy')]\n",
    "\n",
    "# Create a DataFrame for mask files\n",
    "df_mask = pd.DataFrame({\n",
    "    'mask_file': mask_files,\n",
    "})\n",
    "\n",
    "# Extract StudyInstanceUID from mask filenames\n",
    "df_mask['StudyInstanceUID'] = df_mask['mask_file'].apply(lambda x: x.split('_mask.npy')[0])\n",
    "\n",
    "# Create full paths for mask files\n",
    "df_mask['mask_file'] = df_mask['mask_file'].apply(lambda x: os.path.join(data_dir_parent, 'processed_3d', x))\n",
    "\n",
    "# Merge training data with mask data\n",
    "df = df_train.merge(df_mask, on='StudyInstanceUID', how='left')\n",
    "\n",
    "# Function to determine the correct image folder\n",
    "def get_image_folder(study_id):\n",
    "    folder1 = os.path.join(data_dir_parent, 'train_images', study_id)\n",
    "    folder2 = os.path.join(data_dir_parent, 'sfd', 'train_images', study_id)\n",
    "    if os.path.exists(folder1):\n",
    "        return folder1\n",
    "    elif os.path.exists(folder2):\n",
    "        return folder2\n",
    "    else:\n",
    "        return ''  # or return None, depending on how you want to handle missing folders\n",
    "\n",
    "# Add image folder path\n",
    "df['image_folder'] = df['StudyInstanceUID'].apply(get_image_folder)\n",
    "\n",
    "# Fill NA values in mask_file column\n",
    "df['mask_file'] = df['mask_file'].fillna('')\n",
    "\n",
    "# Filter for segmentation data\n",
    "df_seg = df[df['mask_file'] != ''].reset_index(drop=True)\n",
    "\n",
    "# Perform K-Fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "df_seg['fold'] = -1\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(df_seg)):\n",
    "    df_seg.loc[valid_idx, 'fold'] = fold\n",
    "\n",
    "# Display the last few rows of the resulting DataFrame\n",
    "print(df_seg.tail())\n",
    "\n",
    "# Save the DataFrame if needed\n",
    "# df_seg.to_csv(os.path.join(data_dir_parent, 'df_seg.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEGDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.file_list = [f.split('_')[0] for f in os.listdir(data_dir) if f.endswith('_image.npy')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_id = self.file_list[index]\n",
    "        \n",
    "        image_file = os.path.join(self.data_dir, f'{file_id}_image.npy')\n",
    "        mask_file = os.path.join(self.data_dir, f'{file_id}_mask.npy')\n",
    "        \n",
    "        image = np.load(image_file).astype(np.float32)\n",
    "        mask = np.load(mask_file).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            res = self.transform({'image': image, 'mask': mask})\n",
    "            image = res['image']\n",
    "            mask = res['mask']\n",
    "\n",
    "        image = image / 255.\n",
    "        mask = (mask > 127).astype(np.float32)\n",
    "\n",
    "        image = torch.tensor(image).float()\n",
    "        mask = torch.tensor(mask).float()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=1),\n",
    "    transforms.RandFlipd(keys=[\"image\", \"mask\"], prob=0.5, spatial_axis=2),\n",
    "    transforms.RandAffined(keys=[\"image\", \"mask\"], translate_range=[int(x*y) for x, y in zip(image_sizes, [0.3, 0.3, 0.3])], padding_mode='zeros', prob=0.7),\n",
    "    transforms.RandGridDistortiond(keys=(\"image\", \"mask\"), prob=0.5, distort_limit=(-0.01, 0.01), mode=\"nearest\"),    \n",
    "])\n",
    "\n",
    "transforms_valid = transforms.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_dice_score(\n",
    "    y_pred: torch.Tensor,\n",
    "    y_true: torch.Tensor,\n",
    "    threshold: Optional[float] = None,\n",
    "    nan_score_on_empty=False,\n",
    "    eps: float = 1e-7,\n",
    ") -> float:\n",
    "\n",
    "    if threshold is not None:\n",
    "        y_pred = (y_pred > threshold).to(y_true.dtype)\n",
    "\n",
    "    intersection = torch.sum(y_pred * y_true).item()\n",
    "    cardinality = (torch.sum(y_pred) + torch.sum(y_true)).item()\n",
    "\n",
    "    score = (2.0 * intersection) / (cardinality + eps)\n",
    "\n",
    "    has_targets = torch.sum(y_true) > 0\n",
    "    has_predicted = torch.sum(y_pred) > 0\n",
    "\n",
    "    if not has_targets:\n",
    "        if nan_score_on_empty:\n",
    "            score = np.nan\n",
    "        else:\n",
    "            score = float(not has_predicted)\n",
    "    return score\n",
    "\n",
    "\n",
    "def multilabel_dice_score(\n",
    "    y_true: torch.Tensor,\n",
    "    y_pred: torch.Tensor,\n",
    "    threshold=None,\n",
    "    eps=1e-7,\n",
    "    nan_score_on_empty=False,\n",
    "):\n",
    "    ious = []\n",
    "    num_classes = y_pred.size(0)\n",
    "    for class_index in range(num_classes):\n",
    "        iou = binary_dice_score(\n",
    "            y_pred=y_pred[class_index],\n",
    "            y_true=y_true[class_index],\n",
    "            threshold=threshold,\n",
    "            nan_score_on_empty=nan_score_on_empty,\n",
    "            eps=eps,\n",
    "        )\n",
    "        ious.append(iou)\n",
    "\n",
    "    return ious\n",
    "\n",
    "\n",
    "def dice_loss(input, target):\n",
    "    input = torch.sigmoid(input)\n",
    "    smooth = 1.0\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return 1 - ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "\n",
    "def bce_dice(input, target, loss_weights=loss_weights):\n",
    "    loss1 = loss_weights[0] * nn.BCEWithLogitsLoss()(input, target)\n",
    "    loss2 = loss_weights[1] * dice_loss(input, target)\n",
    "    return (loss1 + loss2) / sum(loss_weights)\n",
    "\n",
    "criterion = bce_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(input, truth, clip=[0, 1]):\n",
    "    indices = torch.randperm(input.size(0))\n",
    "    shuffled_input = input[indices]\n",
    "    shuffled_labels = truth[indices]\n",
    "\n",
    "    lam = np.random.uniform(clip[0], clip[1])\n",
    "    input = input * lam + shuffled_input * (1 - lam)\n",
    "    return input, truth, shuffled_labels, lam\n",
    "\n",
    "\n",
    "def train_func(model, loader_train, optimizer, scaler=None):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader_train)\n",
    "    for images, gt_masks in bar:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.cuda()\n",
    "        gt_masks = gt_masks.cuda()\n",
    "\n",
    "        do_mixup = False\n",
    "        if random.random() < p_mixup:\n",
    "            do_mixup = True\n",
    "            images, gt_masks, gt_masks_sfl, lam = mixup(images, gt_masks)\n",
    "\n",
    "        with amp.autocast():\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, gt_masks)\n",
    "            if do_mixup:\n",
    "                loss2 = criterion(logits, gt_masks_sfl)\n",
    "                loss = loss * lam  + loss2 * (1 - lam)\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bar.set_description(f'smth:{np.mean(train_loss[-30:]):.4f}')\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "def valid_func(model, loader_valid):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    outputs = []\n",
    "    ths = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    batch_metrics = [[]] * 7\n",
    "    bar = tqdm(loader_valid)\n",
    "    with torch.no_grad():\n",
    "        for images, gt_masks in bar:\n",
    "            images = images.cuda()\n",
    "            gt_masks = gt_masks.cuda()\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, gt_masks)\n",
    "            valid_loss.append(loss.item())\n",
    "            for thi, th in enumerate(ths):\n",
    "                pred = (logits.sigmoid() > th).float().detach()\n",
    "                for i in range(logits.shape[0]):\n",
    "                    tmp = multilabel_dice_score(\n",
    "                        y_pred=logits[i].sigmoid().cpu(),\n",
    "                        y_true=gt_masks[i].cpu(),\n",
    "                        threshold=0.5,\n",
    "                    )\n",
    "                    batch_metrics[thi].extend(tmp)\n",
    "            bar.set_description(f'smth:{np.mean(valid_loss[-30:]):.4f}')\n",
    "            \n",
    "    metrics = [np.mean(this_metric) for this_metric in batch_metrics]\n",
    "    print('best th:', ths[np.argmax(metrics)], 'best dc:', np.max(metrics))\n",
    "\n",
    "    return np.mean(valid_loss), np.max(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "\n",
    "    log_file = os.path.join(log_dir, f'{kernel_type}.txt')\n",
    "    model_file = os.path.join(model_dir, f'{kernel_type}_fold{fold}_best.pth')\n",
    "\n",
    "    train_ = df_seg[df_seg['fold'] != fold].reset_index(drop=True)\n",
    "    valid_ = df_seg[df_seg['fold'] == fold].reset_index(drop=True)\n",
    "    dataset_train = SEGDataset(train_, 'train', transform=transforms_train)\n",
    "    dataset_valid = SEGDataset(valid_, 'valid', transform=transforms_valid)\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = TimmSegModel(backbone, pretrained=True)\n",
    "    model = convert_3d(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    from_epoch = 0\n",
    "    metric_best = 0.\n",
    "    loss_min = np.inf\n",
    "\n",
    "    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, n_epochs)\n",
    "\n",
    "    print(len(dataset_train), len(dataset_valid))\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        scheduler_cosine.step(epoch-1)\n",
    "\n",
    "        print(time.ctime(), 'Epoch:', epoch)\n",
    "\n",
    "        train_loss = train_func(model, loader_train, optimizer, scaler)\n",
    "        valid_loss, metric = valid_func(model, loader_valid)\n",
    "\n",
    "        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f}.'\n",
    "        print(content)\n",
    "        with open(log_file, 'a') as appender:\n",
    "            appender.write(content + '\\n')\n",
    "\n",
    "        if metric > metric_best:\n",
    "            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "            metric_best = metric\n",
    "\n",
    "        # Save Last\n",
    "        if not DEBUG:\n",
    "            torch.save(\n",
    "                {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
    "                    'score_best': metric_best,\n",
    "                },\n",
    "                model_file.replace('_best', '_last')\n",
    "            )\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m run(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m run(\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[119], line 8\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(fold)\u001b[0m\n\u001b[0;32m      6\u001b[0m train_ \u001b[38;5;241m=\u001b[39m df_seg[df_seg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m fold]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m valid_ \u001b[38;5;241m=\u001b[39m df_seg[df_seg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m fold]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mSEGDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m dataset_valid \u001b[38;5;241m=\u001b[39m SEGDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransforms_valid)\n\u001b[0;32m     10\u001b[0m loader_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset_train, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers)\n",
      "Cell \u001b[1;32mIn[113], line 5\u001b[0m, in \u001b[0;36mSEGDataset.__init__\u001b[1;34m(self, data_dir, transform)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_image.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'train'"
     ]
    }
   ],
   "source": [
    "run(0)\n",
    "run(1)\n",
    "run(2)\n",
    "run(3)\n",
    "run(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
