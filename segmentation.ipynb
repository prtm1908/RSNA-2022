{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import timm\n",
    "from timm.models.layers import Conv2dSame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3dSame(nn.Conv3d):\n",
    "    \"\"\" Tensorflow like 'SAME' convolution wrapper for 3D convolutions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            groups=1,\n",
    "            bias=True,\n",
    "    ):\n",
    "        # Calculate padding for SAME behavior\n",
    "        if isinstance(kernel_size, int):\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        elif isinstance(kernel_size, (tuple, list)):\n",
    "            padding = [(k - 1) // 2 for k in kernel_size]\n",
    "        else:\n",
    "            raise ValueError(\"kernel_size must be int or iterable of int\")\n",
    "\n",
    "        super(Conv3dSame, self).__init__(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride, padding, dilation, groups, bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return conv3d_same(\n",
    "            x, self.weight, self.bias,\n",
    "            self.stride, self.padding, self.dilation, self.groups,\n",
    "        )\n",
    "\n",
    "def conv3d_same(x, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n",
    "    # Here you would need to implement or call the actual 3D convolution logic\n",
    "    # with SAME padding, as PyTorch's native convolution does not have this padding mode.\n",
    "    # This is a placeholder implementation and should be replaced with actual logic.\n",
    "    conv = nn.functional.conv3d(\n",
    "        x, weight, bias, stride, padding, dilation, groups\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = 'data'\n",
    "segmentations_path = os.path.join(base_path, 'segmentations')\n",
    "train_images_path1 = os.path.join(base_path, 'sfd', 'train_images')\n",
    "train_images_path2 = os.path.join(base_path, 'train_images')\n",
    "output_path = os.path.join(base_path, 'processed_3d')\n",
    "\n",
    "backbone='resnet18d'\n",
    "drop_rate = 0.\n",
    "drop_path_rate = 0.\n",
    "n_blocks = 4\n",
    "out_dim = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_image_volume(study_path, target_size=(128, 128, 128)):\n",
    "    # List and sort slice file paths\n",
    "    t_paths = sorted(glob(os.path.join(study_path, \"*\")),\n",
    "                     key=lambda x: int(os.path.basename(x).split('.')[0]))\n",
    "    \n",
    "    # Determine the number of scans and calculate quantile indices\n",
    "    n_scans = len(t_paths)\n",
    "    indices = np.quantile(list(range(n_scans)), np.linspace(0., 1., target_size[2])).round().astype(int)\n",
    "    t_paths = [t_paths[i] for i in indices]\n",
    "    \n",
    "    # Load and process slices\n",
    "    slices = []\n",
    "    for img_path in t_paths:\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((target_size[0], target_size[1]))\n",
    "        slices.append(np.array(img))\n",
    "    \n",
    "    # Stack slices into a 3D volume\n",
    "    volume = np.stack(slices, axis=-1)\n",
    "    \n",
    "    # Normalize and scale the volume\n",
    "    volume = volume - np.min(volume)\n",
    "    volume = volume / (np.max(volume) + 1e-4)\n",
    "    volume = (volume * 255).astype(np.uint8)\n",
    "    \n",
    "    return volume\n",
    "\n",
    "def load_and_process_mask(mask_path, target_size=(128, 128, 128), num_classes=7):\n",
    "    # Load the mask using nibabel\n",
    "    mask_org = nib.load(mask_path).get_fdata()\n",
    "\n",
    "    # Adjust mask orientation if needed\n",
    "    mask_org = mask_org.transpose(1, 0, 2)[::-1, :, ::-1]  # Adjust orientation to (d, w, h)\n",
    "\n",
    "    # Resize mask to target size\n",
    "    if mask_org.shape != target_size:\n",
    "        factors = [t / s for t, s in zip(target_size, mask_org.shape)]\n",
    "        mask_org = zoom(mask_org, factors, order=0)  # Nearest-neighbor interpolation for masks\n",
    "\n",
    "    # Create multi-channel mask\n",
    "    mask = np.zeros((num_classes, target_size[0], target_size[1], target_size[2]))\n",
    "    for cid in range(num_classes):\n",
    "        mask[cid] = (mask_org == (cid + 1))\n",
    "\n",
    "    # Convert mask to [0, 255] and return as uint8\n",
    "    mask = mask.astype(np.uint8) * 255\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    # Get list of study IDs with segmentations\n",
    "    segmentation_ids = [f.split('.')[:-1] for f in os.listdir(segmentations_path) if f.endswith('.nii')]\n",
    "\n",
    "    for study_id_parts in tqdm(segmentation_ids, desc=\"Processing studies\"):\n",
    "        study_id = '.'.join(study_id_parts)\n",
    "        # Check for study folder in both locations\n",
    "        study_path = None\n",
    "        if os.path.exists(os.path.join(train_images_path1, study_id)):\n",
    "            study_path = os.path.join(train_images_path1, study_id)\n",
    "        elif os.path.exists(os.path.join(train_images_path2, study_id)):\n",
    "            study_path = os.path.join(train_images_path2, study_id)\n",
    "        \n",
    "        if study_path is None:\n",
    "            print(f\"Warning: No image folder found for study {study_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Process image volume\n",
    "        image_volume = load_and_process_image_volume(study_path)\n",
    "        \n",
    "        # Process mask\n",
    "        mask_path = os.path.join(segmentations_path, f\"{study_id}.nii\")\n",
    "        mask_volume = load_and_process_mask(mask_path)\n",
    "        \n",
    "        # Save processed data\n",
    "        np.save(os.path.join(output_path, f\"{study_id}_image.npy\"), image_volume)\n",
    "        np.save(os.path.join(output_path, f\"{study_id}_mask.npy\"), mask_volume)\n",
    "\n",
    "    print(\"Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, backbone, segtype='unet', pretrained=False):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            backbone,\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 64, 64))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.Unet.decoder.UnetDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], out_dim, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Unet' has no attribute 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 74\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m module\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module_output\n\u001b[1;32m---> 74\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mTimmSegModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m m \u001b[38;5;241m=\u001b[39m convert_3d(m)\n\u001b[0;32m     76\u001b[0m m(torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m))\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[47], line 17\u001b[0m, in \u001b[0;36mTimmSegModel.__init__\u001b[1;34m(self, backbone, segtype, pretrained)\u001b[0m\n\u001b[0;32m     15\u001b[0m decoder_channels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m16\u001b[39m]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m segtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241m.\u001b[39mUnetDecoder(\n\u001b[0;32m     18\u001b[0m         encoder_channels\u001b[38;5;241m=\u001b[39mencoder_channels[:n_blocks\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     19\u001b[0m         decoder_channels\u001b[38;5;241m=\u001b[39mdecoder_channels[:n_blocks],\n\u001b[0;32m     20\u001b[0m         n_blocks\u001b[38;5;241m=\u001b[39mn_blocks,\n\u001b[0;32m     21\u001b[0m     )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(decoder_channels[n_blocks\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], out_dim, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), padding\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Unet' has no attribute 'decoder'"
     ]
    }
   ],
   "source": [
    "def convert_3d(module):\n",
    "\n",
    "    module_output = module\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module_output = torch.nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_output.weight = module.weight\n",
    "                module_output.bias = module.bias\n",
    "        module_output.running_mean = module.running_mean\n",
    "        module_output.running_var = module.running_var\n",
    "        module_output.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_output.qconfig = module.qconfig\n",
    "            \n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_output = Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_output = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_output.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1,1,1,1,module.kernel_size[0]))\n",
    "\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_output = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_output = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "\n",
    "    for name, child in module.named_children():\n",
    "        module_output.add_module(\n",
    "            name, convert_3d(child)\n",
    "        )\n",
    "    del module\n",
    "\n",
    "    return module_output\n",
    "\n",
    "\n",
    "m = TimmSegModel(backbone)\n",
    "m = convert_3d(m)\n",
    "m(torch.rand(1, 3, 128,128,128)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
